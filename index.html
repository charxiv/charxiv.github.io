<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Chart understanding is crucial for applying Multimodal Large Language Models (MLLMs) to tasks like analyzing scientific papers and financial reports. However, current datasets often use simplified charts with template-based questions, leading to overly optimistic progress assessments. We introduce CharXiv, an evaluation suite with 2,323 diverse and challenging charts from scientific papers. CharXiv includes two question types: (1) descriptive questions on basic chart elements and (2) reasoning questions requiring synthesis of complex visual information. Human experts curated and verified all charts and questions. Our findings show a significant gap in reasoning skills, with the strongest proprietary model (GPT-4o) achieving 47.1% accuracy and the best open-source model (InternVL Chat V1.5) at 29.2%, both far below human performance of 80.5%. CharXiv aims to provide a more realistic measure of MLLM progress in chart understanding.">
  <meta name="keywords" content="Benchmarks, AI, Multimodal Large Language Models. Chart Understanding">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>CharXiv</title>
  
  <script src="https://kit.fontawesome.com/854f31a6c4.js" crossorigin="anonymous"></script>
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">
  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="stylesheet" href="./static/css/dataTables.bulma.css">
  <link rel="icon" href="./static/images/charxiv_logo.png">
</head>


<body>

<!-- paper title and author -->
<section class="section">
  <!-- <div class="hero-body"> -->
    <div class="container is-max-desktop no_max_width">
      <div class="columns is-centered">
        <div class="column has-text-centered is-four-fifths">
          <h1 class="title is-1 publication-title">
            <img src="./static/images/charxiv_logo.png" width="40"/>
            <span class="rainbow_text_animated">CharXiv</span>
          </h1>
          <h2 class="subtitle is-3 publication-subtitle">
            Charting Gaps in Realistic Chart Understanding in Multimodal LLMs 
          </h2>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://zwcolin.github.io">
                <span class="fancy_text_color">Zirui Wang</span>
              </a><sup>1</sup>,
            </span>

            <span class="author-block">
              <a href="https://xiamengzhou.github.io/">
                <span class="fancy_text_color">Mengzhou Xia</span>
              </a><sup>1</sup>,
            </span>

            <span class="author-block">
              <a href="https://x.com/luxihelucy">
                <span class="fancy_text_color">Luxi He</span>
              </a><sup>1</sup>,
            </span>

            <span class="author-block">
              <a href="https://howard50b.github.io/">
                <span class="fancy_text_color">Howard Chen</span>
              </a><sup>1</sup>,
            </span>

            <span class="author-block">
              <a href="https://yitaoliu17.com/">
                <span class="fancy_text_color">Yitao Liu</span>
              </a><sup>3</sup>,
            </span>

            <span class="author-block">
              <a href="https://richardzhu123.github.io/">
                <span class="fancy_text_color">Richard Zhu</span>
              </a><sup>1</sup>,
            </span>

            <span class="author-block">
              <a href="https://kaiquliang.github.io/">
                <span class="fancy_text_color">Kaiqu Liang</span>
              </a><sup>1</sup>,
            </span>

            <span class="author-block">
              <a href="https://xindiwu.github.io/">
                <span class="fancy_text_color">Xindi Wu</span>
              </a><sup>1</sup>,
            </span><br>

            <span class="author-block">
              <a href="https://hliu.cc/">
                <span class="fancy_text_color">Haotian Liu</span>
              </a><sup>2</sup>,
            </span>

            <span class="author-block">
              <a href="https://www.cs.princeton.edu/~smalladi/">
                <span class="fancy_text_color">Sadhika Malladi</span>
              </a><sup>1</sup>,
            </span>

            <span class="author-block">
              <a href="https://x.com/AlexisChvlr">
                <span class="fancy_text_color">Alexis Chevalier</span>
              </a><sup>1</sup>,
            </span>

            <span class="author-block">
              <a href="https://www.cs.princeton.edu/~arora/">
                <span class="fancy_text_color">Sanjeev Arora</span>
              </a><sup>1</sup>,
            </span>

            <span class="author-block">
              <a href="https://www.cs.princeton.edu/~danqic/">
                <span class="fancy_text_color">Danqi Chen</span>
              </a><sup>1</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Princeton Language and Intelligence (PLI), Princeton University</span><br>
            <span class="author-block"><sup>2</sup>University of Wisconsin, Madison</span>
            <span class="author-block"><sup>3</sup>The University of Hong Kong</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- arxiv link -->
              <span class="link-block">
                <a href="http://arxiv.org/abs/2406.18521"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/princeton-nlp/CharXiv"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Data Link. -->
              <span class="link-block">
                <a href="https://huggingface.co/datasets/princeton-nlp/CharXiv"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fa fa-database"></i>
                  </span>
                  <span>Data</span>
                </a>
              </span>
              <!-- Leaderboard Link. -->
              <span class="link-block">
                <a href="#leaderboard"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fa fa-trophy"></i>
                  </span>
                  <span>Leaderboard</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
      <!-- teaser video -->
      <div class="columns is-centered">
        <div class="column is-four-fifths">
          <video id="teaser" autoplay muted width="90%" style="width: 60%;" controls>
            <source src="static/videos/CharXiv_MV_H265.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
      <div class="columns is-centered">
        <div class="column is-four-fifths">
          <h3 class="subtitle has-text-centered">
            Watch the 80-second music video to learn the motivation and key findings of CharXiv!<br>
            (Lyrics by <a href="https://openai.com/index/hello-gpt-4o/">GPT-4o</a>  from the abstract and Music by <a href="https://suno.com/">Suno</a>)
          </h3>
        </div>
      </div>
    </div>
</section>

<!-- Abstract -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">
          <span class="fancy_text_color">Introduction</span>
        </h2>
        <div style="margin-bottom: 1em;">
          <p>
            Chart understanding plays a pivotal role when applying Multimodal Large Language Models (MLLMs) to real-world tasks such as analyzing scientific papers or financial reports. However, existing datasets often focus on oversimplified and homogeneous charts with template-based questions, leading to an over-optimistic measure of progress. In this work, we propose <span class="fancy_text_color">CharXiv</span>, a comprehensive evaluation suite involving 2,323 natural, challenging, and diverse charts from scientific papers. <span class="fancy_text_color">CharXiv</span> includes two types of questions: (1) descriptive questions about examining basic chart elements and (2) reasoning questions that require synthesizing information across complex visual elements in the chart. To ensure quality, all charts and questions are handpicked, curated, and verified by human experts. Our results reveal a substantial, previously underestimated gap between the reasoning skills of the strongest proprietary model (i.e., GPT-4o), which achieves 47.1% accuracy, and the strongest open-source model (i.e., InternVL Chat V1.5), which achieves 29.2%. All models lag far behind human performance of 80.5%, underscoring weaknesses in the chart understanding capabilities of existing MLLMs. We hope <span class="fancy_text_color">CharXiv</span> facilitates future research on MLLM chart understanding by providing a more realistic and faithful measure of progress.
          </p>
        </div>
        <div class="has-text-centered">
          <img src="./static/images/comparison.png">
        </div>
        <div class="has-text-centered">
          <p>
            <b>Figure:</b> Many open-source models surpass proprietary model performance on existing benchmarks (subsets of DVQA, FigureQA and ChartQA from MathVista) yet fail consistently in reasoning questions from <span class="fancy_text_color">CharXiv
          </p>
        </div>
      
        <br>
        <h2 class="title is-3" id="leaderboard"><span class="fancy_text_color">
          Leaderboard</span>
        </h2>
        <div class="content">
          <div class="content has-text-justified">
            <p>
              We evaluate general-purpose MLLMs on CharXiv and provide a leaderboard for the community to track progress. Note that all models are evaluated in a zero-shot setting with a set of natural instructions for each question type. The numbers below are based on the model performance on the validation set, which consists of 1,000 charts and 5,000 questions in total.
            </p>
            <div class="has-text-centered">
              <div id="table-container"></div>
            </div>
          </div>
        </div>
      
        <div class="is-size-5 publication-authors">
          <span class="author-block" style="font-size: 120%;"><i>The website is being constructed. We will roll out all the update for this project in the coming days!</i></span>
          <div class="has-text-centered">
            <img src="./static/images/charxiv_logo.png", width="200"/>
          </div>
        </div>
      </div>
    </div>
    <br>
  </div>
</section>

<br><br>

<section class="section no_pad_section">
  <div class="container is-max-desktop content">
    <div class="columns is-centered has-text-centered">

      <!-- <div></div> -->
      <h2 class="title is-four-fifths">
        <span class="fancy_text_color">Citation</span>
      </h2>
    </div>

    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <pre style="border-radius: 25px;">
          <code>@article{wang2024charxiv,
  title={CharXiv: Charting Gaps in Realistic Chart Understanding in Multimodal LLMs},
  author={Wang, Zirui and Xia, Mengzhou and He, Luxi and Chen, Howard and Liu, Yitao and Zhu, Richard and Liang, Kaiqu and Wu, Xindi and Liu, Haotian and Malladi, Sadhika and Chevalier, Alexis and Arora, Sanjeev and Chen, Danqi},
  journal={arXiv preprint arXiv:2406.18521},
  year={2024}
}</code>
        </pre>
      </div>
    </div>
      
  </div>
</section>

<footer class="footer" style="padding-bottom: 1.5em; padding-top: 1.5em;">
  <div class="container">
    <div class="content has-text-centered">
      <span>Website based on <a href="https://nerfies.github.io">Nerfies, <a href="https://mlpc-ucsd.github.io/TokenCompose/">TokenCompose, <a href="https://mmmu-benchmark.github.io/">MMMU</a>, and <a href="https://mathvista.github.io/">MathVista</a>.</span>


    </div>
  </div>
</footer>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>

  <script src="https://code.jquery.com/jquery-3.3.1.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/4.2.1/js/bootstrap.bundle.min.js"></script>
  <script src="./static/js/jquery.csv.min.js"></script>
  <script src="https://cdn.datatables.net/2.0.8/js/dataTables.min.js"></script>
  <script src="https://cdn.datatables.net/2.0.8/js/dataTables.bulma.min.js"></script>
  <script src="./static/js/csv_to_html_table.js"></script>
  <script>
    CsvToHtmlTable.init({
      csv_path: 'data/val_result.csv', 
      element: 'table-container', 
      allow_download: true,
      csv_options: {separator: ',', delimiter: '"'},
      datatables_options: {
        "paging": false, 
        "order": [[3, 'desc']],
        "columnDefs": [
        {targets: [0], className: 'dt-left', className: 'dt-head-left'},
        ]
      }
    });
  </script>
</body>
</html>
